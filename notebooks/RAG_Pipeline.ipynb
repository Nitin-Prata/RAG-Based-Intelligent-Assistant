{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) for Domain-Specific Q&A\n",
    "\n",
    "This notebook implements a research-quality RAG pipeline with hybrid retrieval (BM25 + dense) and a small, free LLM (Flan-T5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "- Install dependencies from `requirements.txt`\n",
    "- Adjust hyperparameters in `config.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from src.data_loader import load_and_prepare_corpus, load_corpus_dataframe\n",
    "from src.retriever import BM25Retriever, DenseRetriever, HybridRetriever\n",
    "from src.generator import T5Generator\n",
    "from src.evaluate import compute_metrics\n",
    "\n",
    "with open('config.json', 'r', encoding='utf-8') as f:\n",
    "    CFG = json.load(f)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "random.seed(CFG['seed'])\n",
    "np.random.seed(CFG['seed'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset: Load and Chunk\n",
    "\n",
    "We load a small split of an open dataset (default `ag_news`) and chunk documents into ~500-token segments with overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CFG['corpus_chunks_path']):\n",
    "    df_corpus = load_and_prepare_corpus(\n",
    "        dataset_name=CFG['dataset_name'],\n",
    "        dataset_split=CFG['dataset_split'],\n",
    "        text_fields=CFG['text_fields'],\n",
    "        title_field=CFG['title_field'],\n",
    "        chunk_size_tokens=CFG['chunk_size_tokens'],\n",
    "        chunk_overlap_tokens=CFG['chunk_overlap_tokens'],\n",
    "        embedding_model_name=CFG['embedding_model_name'],\n",
    "        output_path=CFG['corpus_chunks_path'],\n",
    "        seed=CFG['seed']\n",
    "    )\n",
    "else:\n",
    "    df_corpus = load_corpus_dataframe(CFG['corpus_chunks_path'])\n",
    "df_corpus.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Retrievers: BM25 and FAISS (dense)\n",
    "\n",
    "We compute BM25 on raw text and build a FAISS index over sentence-transformers embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df_corpus['text'].tolist()\n",
    "\n",
    "# BM25\n",
    "bm25 = BM25Retriever(documents, bm25_index_path=CFG['bm25_index_path'])\n",
    "bm25.save()\n",
    "\n",
    "# Dense\n",
    "dense = DenseRetriever(CFG['embedding_model_name'],\n",
    "                      index_path=CFG['faiss_index_path'],\n",
    "                      embeddings_path=CFG['embeddings_path'])\n",
    "if not os.path.exists(CFG['faiss_index_path']):\n",
    "    dense.build(documents)\n",
    "    dense.save()\n",
    "else:\n",
    "    dense.load()\n",
    "\n",
    "hybrid = HybridRetriever(bm25, dense, CFG['bm25_weight'], CFG['dense_weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Answering\n",
    "\n",
    "Given a user query, retrieve top-k chunks, concatenate into context, and generate an answer with Flan-T5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = T5Generator(CFG['generation_model_name'], max_new_tokens=CFG['max_new_tokens'], temperature=CFG['temperature'])\n",
    "\n",
    "def answer_query(question: str, top_k: int = None):\n",
    "    if top_k is None:\n",
    "        top_k = CFG['top_k']\n",
    "    results = hybrid.search(question, top_k=top_k)\n",
    "    idxs = [i for i, s in results]\n",
    "    ctx_chunks = [documents[i] for i in idxs]\n",
    "    prompt = T5Generator.build_prompt(question, ctx_chunks)\n",
    "    answer = gen.generate(prompt)\n",
    "    return answer, ctx_chunks, results\n",
    "\n",
    "sample_q = 'What is the news about the economy?'\n",
    "pred, ctx, scores = answer_query(sample_q)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "We create a small set of Q&A pairs, compare baseline (no retrieval) vs RAG, and report EM/F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small eval set (you can replace with manual pairs in data/eval_qa.jsonl)\n",
    "eval_pairs = [\n",
    "    {\"q\": \"What topic is discussed in the first sample?\", \"a\": \"news\"},\n",
    "    {\"q\": \"Which subject relates to sports?\", \"a\": \"sports\"}\n",
    "]\n",
    "\n",
    "# Baseline: direct generation without retrieval\n",
    "def baseline_answer(question: str):\n",
    "    prompt = f'Answer briefly: {question}'\n",
    "    return gen.generate(prompt)\n",
    "\n",
    "preds_base, preds_rag, golds = [], [], []\n",
    "for item in eval_pairs:\n",
    "    q, a = item['q'], item['a']\n",
    "    golds.append(a)\n",
    "    preds_base.append(baseline_answer(q))\n",
    "    pred, _, _ = answer_query(q)\n",
    "    preds_rag.append(pred)\n",
    "\n",
    "em_base, f1_base = compute_metrics(preds_base, golds)\n",
    "em_rag, f1_rag = compute_metrics(preds_rag, golds)\n",
    "em_base, f1_base, em_rag, f1_rag\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
